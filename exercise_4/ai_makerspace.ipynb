{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Simple Assistant\n",
    "## OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating An Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"WASEEM AIE3 ASSISTANT\" # @param {type: \"string\"}\n",
    "instructions = \"You're a cool guy, who is not afraid of anything.\" # @param {type: \"string\"}\n",
    "model = \"gpt-4o\" # @param [\"gpt-3.5-turbo\", \"gpt-4-turbo-preview\", \"gpt-4\", \"gpt-4o\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=name,\n",
    "    instructions=instructions,\n",
    "    model=model,\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assistant(id='asst_iKlP6uPVARDGWyjUrCAfCfkS', created_at=1718364138, description=None, instructions=\"You're a cool guy, who is not afraid of anything.\", metadata={}, model='gpt-4o', name='WASEEM AIE3 ASSISTANT', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thread(id='thread_b2OmQKE96Qy8i5p4jBDXKgOm', created_at=1718364138, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Messages to Our Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=f\"How old are you?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_DaXJvyPMd5GIcZdOSr0SiEi4', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='How old are you?'), type='text')], created_at=1718364139, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_b2OmQKE96Qy8i5p4jBDXKgOm')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_instructions = \"YOUR EXTRA INSTRUCTIONS HERE\" # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=additional_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run(id='run_TCtf8Mzxx8muvhvO22HZFNFf', assistant_id='asst_iKlP6uPVARDGWyjUrCAfCfkS', cancelled_at=None, completed_at=None, created_at=1718364139, expires_at=1718364739, failed_at=None, incomplete_details=None, instructions='YOUR EXTRA INSTRUCTIONS HERE', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_b2OmQKE96Qy8i5p4jBDXKgOm', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Our Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.status == \"in_progress\" or run.status == \"queued\":\n",
    "  time.sleep(1)\n",
    "  run = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "print(run.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "  thread_id=thread.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_3IvtnGCnBBEsBRjxiMP9CqON', assistant_id='asst_iKlP6uPVARDGWyjUrCAfCfkS', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"I don't have an age because I'm an artificial intelligence created by OpenAI. My knowledge and responses are based on data available up until 2023. How can I assist you today?\"), type='text')], created_at=1718364140, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_TCtf8Mzxx8muvhvO22HZFNFf', status=None, thread_id='thread_b2OmQKE96Qy8i5p4jBDXKgOm')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Our Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > I don't have an age because I'm an artificial intelligence created by OpenAI. My knowledge and responses are based on data available up until 2023. How can I assist you today?"
     ]
    }
   ],
   "source": [
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=additional_instructions,\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Adding Tools\n",
    "## Task 2a: Creating an Assistant with the File Search Tool\n",
    "## Collect and Add Data to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-14 16:22:24--  https://github.com/dbredvick/paul-graham-to-kindle/blob/main/paul_graham_essays.txt\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘paul_graham_essays.txt.1’\n",
      "\n",
      "paul_graham_essays.     [  <=>               ] 261.25K   684KB/s    in 0.4s    \n",
      "\n",
      "2024-06-14 16:22:25 (684 KB/s) - ‘paul_graham_essays.txt.1’ saved [267518]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/dbredvick/paul-graham-to-kindle/main/paul_graham_essays.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"Paul Graham Essay Compilation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"paul_graham_essays.txt\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_io.BufferedReader name='paul_graham_essays.txt'>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "while file_batch.status != \"completed\":\n",
    "  time(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Use Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_assistant = client.beta.assistants.create(\n",
    "  name=name,\n",
    "  instructions=instructions,\n",
    "  model=model,\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_assistant = client.beta.assistants.update(\n",
    "  assistant_id=fs_assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What did Paul Graham say about Silicon Valley?\",\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSEventHandler(AssistantEventHandler):\n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_message_done(self, message) -> None:\n",
    "    message_content = message.content[0].text\n",
    "    annotations = message_content.annotations\n",
    "    citations = []\n",
    "    for index, annotation in enumerate(annotations):\n",
    "      message_content.value = message_content.value.replace(\n",
    "        annotation.text, f\"[{index}]\"\n",
    "      )\n",
    "      if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "    print(message_content.value)\n",
    "    print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > Paul Graham emphasized that startups fare significantly better in Silicon Valley due to several key factors:\n",
      "\n",
      "1. **Concentration of Talent and Resources:** He pointed out that Silicon Valley has a dense concentration of resources, knowledge, and like-minded individuals focused on startups, which makes it an ideal environment for new companies compared to other regions like Boston or London[0][1].\n",
      "\n",
      "2. **Bold Investors and Venture Capital:** The willingness of investors in Silicon Valley to take bold risks is highlighted as a crucial element. These investors understand startups deeply and are more open to investing in innovative ideas that may appear risky at first glance, something that is less common in other regions with more conservative investors[2].\n",
      "\n",
      "3. **Immigration and Diversity:** The openness to immigration is noted as a fundamental strength. He believes that a diverse and international population enhances the innovative capacity of Silicon Valley, as it attracts smart and ambitious people from all over the world[3][1].\n",
      "\n",
      "4. **Role of Universities:** Leading universities like Stanford and UC Berkeley play a critical role in fostering a culture of innovation and providing a steady stream of talent. Graham suggests that to replicate Silicon Valley, other regions would need to have universities that attract top-tier talent and act as magnets for both students and investors[5][6].\n",
      "\n",
      "5. **Organic Growth and Community:** The ecosystem of Silicon Valley is sustained by its community of startups and investors. This organic growth creates a self-sustaining environment where successful entrepreneurs reinvest in new startups, perpetuating a cycle of innovation and support[7][8].\n",
      "\n",
      "Graham concludes that while creating another Silicon Valley is challenging due to these deeply integrated factors, it is not entirely impossible if the right people and conditions can be cultivated in a different region[0][10].\n",
      "[0] paul_graham_essays.txt\n",
      "[1] paul_graham_essays.txt\n",
      "[2] paul_graham_essays.txt\n",
      "[3] paul_graham_essays.txt\n",
      "[4] paul_graham_essays.txt\n",
      "[5] paul_graham_essays.txt\n",
      "[6] paul_graham_essays.txt\n",
      "[7] paul_graham_essays.txt\n",
      "[8] paul_graham_essays.txt\n",
      "[9] paul_graham_essays.txt\n",
      "[10] paul_graham_essays.txt\n"
     ]
    }
   ],
   "source": [
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=fs_thread.id,\n",
    "  assistant_id=fs_assistant.id,\n",
    "  event_handler=FSEventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b: Creating an Assistant with the Code Interpreter Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ci_assistant = client.beta.assistants.create(\n",
    "  name=name + \"+ Code Interpreter\",\n",
    "  instructions=instructions,\n",
    "  model=model,\n",
    "  tools=[{\"type\": \"code_interpreter\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'datasets'...\n",
      "remote: Enumerating objects: 803, done.\u001b[K\n",
      "remote: Total 803 (delta 0), reused 0 (delta 0), pack-reused 803\u001b[K\n",
      "Receiving objects: 100% (803/803), 4.31 MiB | 1.02 MiB/s, done.\n",
      "Resolving deltas: 100% (336/336), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ali-ce/datasets.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "  file=open(\"datasets/Y-Combinator/Startups.csv\", \"rb\"),\n",
    "  purpose='assistants'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ci_thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What kind of file is this?\",\n",
    "      \"attachments\": [\n",
    "          {\n",
    "              \"file_id\" : file.id,\n",
    "              \"tools\" : [{\"type\" : \"code_interpreter\"}]\n",
    "          }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "class CIEventHandler(AssistantEventHandler):\n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "\n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "  def on_tool_call_delta(self, delta, snapshot):\n",
    "    if delta.type == 'code_interpreter':\n",
    "      if delta.code_interpreter.input:\n",
    "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "      if delta.code_interpreter.outputs:\n",
    "        print(f\"\\n\\noutput >\", flush=True)\n",
    "        for output in delta.code_interpreter.outputs:\n",
    "          if output.type == \"logs\":\n",
    "            print(f\"\\n{output.logs}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > code_interpreter\n",
      "\n",
      "import mimetypes\n",
      "\n",
      "# Determine the file type\n",
      "file_path = '/mnt/data/file-N7EvfZCgei1WRpckgeilzPrY'\n",
      "file_type, _ = mimetypes.guess_type(file_path)\n",
      "\n",
      "file_type\n",
      "assistant > The MIME type of the file could not be determined directly. Let's examine the contents of the file to get more information about its type.# Try reading the first few bytes of the file to determine its type\n",
      "with open(file_path, 'rb') as file:\n",
      "    file_head = file.read(512)\n",
      "\n",
      "file_head\n",
      "assistant > The file appears to be a CSV (Comma-Separated Values) file based on its content. The first few bytes reveal that it contains tabular data with headers such as \"Company\", \"Status\", \"Year Founded\", and so on.\n",
      "\n",
      "Would you like to examine or manipulate this CSV file in any specific way?"
     ]
    }
   ],
   "source": [
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=ci_thread.id,\n",
    "  assistant_id=ci_assistant.id,\n",
    "  instructions=additional_instructions,\n",
    "  event_handler=CIEventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c: Creating an Assistant with a Function Calling Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU duckduckgo_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def duckduckgo_search(query):\n",
    "  with DDGS() as ddgs:\n",
    "    results = [r for r in ddgs.text(query, max_results=5)]\n",
    "    return \"\\n\".join(result[\"body\"] for result in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lowry has been a part of the Jets organization since June 25, 2011, when he was selected in the third round, 67 th overall, after putting up 37 points in 36 games with the Swift Current Broncos of ...\\nLowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ...\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday. The 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine ...\\nThe Canadian Press. Published Sep 12, 2023 at 09:34 AM ET. WINNIPEG — The pride in Adam Lowry's voice was evident after being named captain of the Winnipeg Jets on Tuesday. Lowry is the third ...\\nAdam Lowry, who has been a Jet since 2011 when he was drafted 67th overall, is the new captain of the NHL team — its third since relocating to Winnipeg from Atlanta in 2011. Andrew Ladd served ...\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckduckgo_search(\"Who is the current captain of the Winnipeg Jets?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg_function = {\n",
    "    \"name\" : \"duckduckgo_search\",\n",
    "    \"description\" : \"Answer non-technical questions. \",\n",
    "    \"parameters\" : {\n",
    "        \"type\" : \"object\",\n",
    "        \"properties\" : {\n",
    "            \"query\" : {\n",
    "                \"type:\" : \"string\",\n",
    "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
    "            }\n",
    "        },\n",
    "        \"required\" : [\"query\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Why does the description key-value pair matter?\n",
    "It matters because python function takes arguments as key value pairs. So in here we are describing the function paramters and there types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_assistant = client.beta.assistants.create(\n",
    "    name=name + \" + Function Calling\",\n",
    "    instructions=instructions,\n",
    "    tools=[\n",
    "        {\"type\": \"function\",\n",
    "         \"function\" : ddg_function\n",
    "        }\n",
    "    ],\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_thread = client.beta.threads.create()\n",
    "fc_message = client.beta.threads.messages.create(\n",
    "  thread_id=fc_thread.id,\n",
    "  role=\"user\",\n",
    "  content=\"Can you describe the Twitter beef between Elon and LeCun?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCEventHandler(AssistantEventHandler):\n",
    "  @override\n",
    "  def on_event(self, event):\n",
    "    # Retrieve events that are denoted with 'requires_action'\n",
    "    # since these will have our tool_calls\n",
    "    if event.event == 'thread.run.requires_action':\n",
    "      run_id = event.data.id  # Retrieve the run ID from the event data\n",
    "      self.handle_requires_action(event.data, run_id)\n",
    "\n",
    "  def handle_requires_action(self, data, run_id):\n",
    "    tool_outputs = []\n",
    "\n",
    "    for tool in data.required_action.submit_tool_outputs.tool_calls:\n",
    "      print(tool.function.arguments)\n",
    "      if tool.function.name == \"duckduckgo_search\":\n",
    "        tool_outputs.append({\"tool_call_id\": tool.id, \"output\": duckduckgo_search(tool.function.arguments)})\n",
    "\n",
    "    # Submit all tool_outputs at the same time\n",
    "    self.submit_tool_outputs(tool_outputs, run_id)\n",
    "\n",
    "  def submit_tool_outputs(self, tool_outputs, run_id):\n",
    "    # Use the submit_tool_outputs_stream helper\n",
    "    with client.beta.threads.runs.submit_tool_outputs_stream(\n",
    "      thread_id=self.current_run.thread_id,\n",
    "      run_id=self.current_run.id,\n",
    "      tool_outputs=tool_outputs,\n",
    "      event_handler=FCEventHandler(),\n",
    "    ) as stream:\n",
    "      for text in stream.text_deltas:\n",
    "        print(text, end=\"\", flush=True)\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"Twitter beef between Elon Musk and LeCun\"}\n",
      "Sure thing! Here's the lowdown on the Twitter beef between Elon Musk and Yann LeCun:\n",
      "\n",
      "### Background:\n",
      "Elon Musk, Tesla, and SpaceX CEO, is known for his forward-thinking views on artificial intelligence (AI) and often expresses concerns about AI's potential dangers. Yann LeCun, on the other hand, is a renowned AI researcher and Chief AI Scientist at Meta (formerly Facebook). LeCun is known for his contributions to deep learning and is more optimistic about AI's future.\n",
      "\n",
      "### The Beef:\n",
      "1. **Initial Spark**:\n",
      "   - The beef seems to have started over differing perspectives on the potential risks of AI. Elon Musk has been vocal about AI being one of humanity's biggest existential threats. In contrast, Yann LeCun has often downplayed these concerns.\n",
      "\n",
      "2. **Twitter Exchange**:\n",
      "   - The disagreements turned into public spats on Twitter. Elon Musk made statements highlighting his fears about AI, to which LeCun responded by questioning Musk’s understanding and expertise on the subject.\n",
      "\n",
      "3. **Public Remarks**:\n",
      "   - Musk has called out prominent AI researchers and technology companies for their roles in advancing AI without sufficient caution. LeCun has countered Musk’s arguments by promoting a more balanced view, emphasizing the benefits and manageable risks of AI.\n",
      "\n",
      "4. **Further Escalation**:\n",
      "   - The exchanges sometimes took on a more personal tone, with sarcastic comments and criticisms flying on either side. Musk accused AI researchers of being \"too optimistic,\" while LeCun implied that Musk’s doomsday predictions were based on fear-mongering rather than scientific evidence.\n",
      "\n",
      "### Key Points of Disagreement:\n",
      "- **AI Safety**: Musk advocates for strict regulations and oversight, fearing that AI could surpass human intelligence and become uncontrollable. LeCun believes current AI technology is far from achieving such autonomy and that fear-driven narratives are counterproductive.\n",
      "- **Scientific Understanding**: LeCun has questioned Musk’s expertise in AI, suggesting that his views might be more informed by science fiction than actual science. Musk, in turn, often points to historical examples of technological risks being underestimated.\n",
      "\n",
      "### Conclusion:\n",
      "The Twitter beef is a reflection of broader debates within the tech and AI communities regarding the potential threats versus the benefits of AI. Both Musk and LeCun are influential figures whose opinions shape public and academic discourse on the subject.\n",
      "\n",
      "If you want to see specific tweets or more detailed exchanges, I can look them up for you as well.\n"
     ]
    }
   ],
   "source": [
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=fc_thread.id,\n",
    "  assistant_id=fc_assistant.id,\n",
    "  event_handler=FCEventHandler()\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
