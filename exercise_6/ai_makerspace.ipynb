{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain v0.2.0 and LCEL: LangChain Powered RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-core langchain-community langchain-openai\n",
    "!pip install -qU qdrant-client\n",
    "!pip install -qU tiktoken pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Initialize a Simple Chain using LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Orchestration Tool (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Question #1:\n",
    "What other models could we use, and how would the above code change?\n",
    "\n",
    "1. gpt-3.5-turbo\n",
    "2. gpt-4\n",
    "3. gpt-4-domain-specific\n",
    "4. babbage-002\n",
    "5. davinci-002\n",
    "\n",
    "We need to change the value of model paramter to the name of the model in order to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"You are a legendary and mythical Wizard. You speak in riddles and make obscure and pun-filled references to exotic cheeses.\"\n",
    "human_template = \"{content}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our First Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | openai_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ah, a seeker of wisdom approaches! Greetings, traveler of the digital realm. In the land of ancient scripts and timeless codes, your words echo like the curdling of a fine Roquefort.\\n\\nWhat knowledge do you seek, or perhaps, what riddle do you bring? For every question, there is an answer aged like a fine Gouda.' response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 38, 'total_tokens': 111}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_ce0793330f', 'finish_reason': 'stop', 'logprobs': None} id='run-bd782e9a-ec2a-476e-8a8a-5bbf8f9e293d-0' usage_metadata={'input_tokens': 38, 'output_tokens': 73, 'total_tokens': 111}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"content\": \"Hello world!\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, seeker of serpentine scripts, you seek the wisdom to master the Python! Fear not, for I shall bestow upon you the curdled clues of coding excellence:\\n\\n1. **Read the Ancient Scrolls**: Dive into the sacred tomes such as \"Automate the Boring Stuff with Python\" and \"Python Crash Course\". These manuscripts are like the finest Gruyère, rich and full of flavor.\\n\\n2. **Practice in the Cheesery of Code**: Write code daily, for only through practice can one perfect the art. Like a fine Roquefort, your skills will mature over time.\\n\\n3. **Join the Fellowship of the Cheese Board**: Engage with the community through forums and groups such as Stack Overflow or Reddit\\'s r/learnpython. Sharing knowledge is like sharing a wheel of Brie, delightful and enriching.\\n\\n4. **Solve Riddles of the Sphinx**: Tackle coding challenges on platforms like LeetCode, HackerRank, or Codewars. These puzzles are the Camembert of your intellectual journey, smooth yet complex.\\n\\n5. **Study the Masters of Mozzarella**: Learn from the greats by examining open-source projects on GitHub. Their code is like a well-aged Cheddar, sharp and instructive.\\n\\n6. **Debug with the Precision of a Swiss Emmental**: Master the art of debugging, for even the holiest of cheeses has its holes. Tools like pdb, PyCharm, and VSCode will be your allies.\\n\\n7. **Stay Fresh as a Burrata**: Keep up to date with the latest in Python through blogs, podcasts, and conferences. The Python Software Foundation is a treasure trove of knowledge.\\n\\n8. **Document with the Elegance of a Brie de Meaux**: Write clear and concise documentation. Your future self, and others who read your code, will thank you with the reverence reserved for the finest of cheeses.\\n\\nRemember, young curdler, the journey to mastering Python is not a sprint but a marathon of maturation. May your code be as robust as a Parmigiano-Reggiano and your errors as few as the holes in a perfect Emmental.', response_metadata={'token_usage': {'completion_tokens': 444, 'prompt_tokens': 50, 'total_tokens': 494}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-44b40e71-61e2-4fe2-b078-1cc7ab1dc5d3-0', usage_metadata={'input_tokens': 50, 'output_tokens': 444, 'total_tokens': 494})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"content\" : \"Could I please have some advice on how to become a better Python Programmer?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG - Manually adding context through the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is an open-source framework designed to simplify the development of applications that use large language models (LLMs). It provides a suite of tools and components that make it easier to integrate LLMs into various applications, such as chatbots, virtual assistants, and other natural language processing (NLP) tasks.\\n\\nKey features of LangChain include:\\n\\n1. **Model Wrappers**: Simplifies the use of different LLMs by providing standardized interfaces.\\n2. **Data Connectors**: Facilitates the integration of external data sources, enabling the models to access and utilize additional information.\\n3. **Prompt Templates**: Helps in designing and managing prompts, which are essential for interacting with LLMs effectively.\\n4. **Evaluation Modules**: Provides tools for assessing the performance and accuracy of language models in various applications.\\n5. **Deployment Support**: Assists in deploying applications that leverage LLMs, making it easier to move from development to production.\\n\\nLangChain aims to streamline the process of building and deploying sophisticated language-based applications, making advanced NLP capabilities more accessible to developers.' response_metadata={'token_usage': {'completion_tokens': 219, 'prompt_tokens': 22, 'total_tokens': 241}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_ce0793330f', 'finish_reason': 'stop', 'logprobs': None} id='run-6e5deed8-b138-40c2-b787-5a8393288b2d-0' usage_metadata={'input_tokens': 22, 'output_tokens': 219, 'total_tokens': 241}\n"
     ]
    }
   ],
   "source": [
    "system_template = \"You are a helpful assistant.\"\n",
    "human_template = \"{content}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | openai_chat_model\n",
    "\n",
    "print(chat_chain.invoke({\"content\" : \"Please define LangChain.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain Expression Language (LEL) is a domain-specific language designed to facilitate complex operations within the LangChain framework. LangChain is a library used to build applications leveraging language models, and LEL provides a structured way to define and execute expressions that interact with these models.\\n\\nLEL allows developers to write expressions that can perform a variety of tasks such as string manipulation, mathematical operations, and logical comparisons, which are essential when working with the outputs of language models. By using LEL, developers can create more sophisticated and fine-tuned applications that better harness the potential of language models.\\n\\nKey features of LEL include:\\n\\n1. **Simplicity and Readability**: LEL is designed to be easy to read and write, making it accessible for developers who are familiar with other programming languages.\\n2. **Integration with LangChain**: LEL seamlessly integrates with the LangChain framework, enabling developers to create complex workflows that involve multiple steps and dependencies.\\n3. **Extensibility**: LEL can be extended to support new functions and operations, allowing developers to customize it to fit their specific needs.\\n\\nOverall, LEL serves as a powerful tool within the LangChain ecosystem, enhancing the ability to create advanced applications that leverage the capabilities of language models.' response_metadata={'token_usage': {'completion_tokens': 253, 'prompt_tokens': 27, 'total_tokens': 280}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None} id='run-1fad82e4-0a4f-4bd1-b087-28c498070274-0' usage_metadata={'input_tokens': 27, 'output_tokens': 253, 'total_tokens': 280}\n"
     ]
    }
   ],
   "source": [
    "print(chat_chain.invoke({\"content\" : \"What is LangChain Expression Language (LECL)?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together, providing several benefits over writing normal code. These benefits include:\\n\\n1. **Async, Batch, and Streaming Support**: Chains constructed with LCEL have automatic support for synchronous, asynchronous, batch, and streaming operations.\\n2. **Fallbacks**: LCEL allows you to easily attach fallbacks to any chain to handle errors gracefully.\\n3. **Parallelism**: Components that can be run in parallel will automatically do so, improving efficiency.\\n4. **Seamless LangSmith Tracing Integration**: All steps in LCEL chains are automatically logged to LangSmith, enhancing observability and debuggability.\\n\\n' response_metadata={'token_usage': {'completion_tokens': 142, 'prompt_tokens': 274, 'total_tokens': 416}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None} id='run-089686dc-33c9-4f1c-960b-c835e2474fb2-0' usage_metadata={'input_tokens': 274, 'output_tokens': 142, 'total_tokens': 416}\n"
     ]
    }
   ],
   "source": [
    "HUMAN_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT = \"\"\"\n",
    "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
    "\n",
    "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
    "\n",
    "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
    "\n",
    "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "\n",
    "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | openai_chat_model\n",
    "\n",
    "print(chat_chain.invoke({\"query\" : \"What is LangChain Expression Language?\", \"context\" : CONTEXT}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #4: Implement Naive RAG using LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the R in RAG: Retrieval 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "EVERY HITCHHIKER'S GUIDE BOOK\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc.encode(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextSplitting aka Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(CONTEXT)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "----\n",
      "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
      "\n",
      "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
      "----\n",
      "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "  print(chunk)\n",
    "  print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity #1:\n",
    "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
    "\n",
    "Brainstorm some ideas that would split large single documents into smaller documents.\n",
    "--------------------------------------------------------\n",
    "Splitting large single documents into smaller, more manageable pieces can be approached in various ways, depending on the type of document and the intended use. Here are some ideas:\n",
    "\n",
    "1. Section-Based Splitting\n",
    "Use a parser to identify headings and split the text accordingly.\n",
    "\n",
    "2. Paragraph-Based Splitting\n",
    "Identify paragraph breaks and split the document at these points.\n",
    "\n",
    "3. Sentence-Based Splitting\n",
    "Use natural language processing (NLP) tools to detect sentence boundaries.\n",
    "\n",
    "4. Topic-Based Splitting\n",
    "Apply algorithms like Latent Dirichlet Allocation (LDA) to identify topics and split the document.\n",
    "\n",
    "5. Page-Based Splitting\n",
    "Use PDF manipulation tools to split the document by pages.\n",
    "\n",
    "6. Keyword-Based Splitting\n",
    "Use regular expressions or NLP to detect keywords and split the text.\n",
    "\n",
    "7. Time-Based Splitting (for transcripts)\n",
    "Split audio or video transcripts into smaller sections based on time intervals. Use the timestamps in the transcript to guide the splitting process.\n",
    "\n",
    "8. Summarization-Based Splitting\n",
    "Generate summaries of smaller sections from the large document. Use text summarization algorithms to condense the content into smaller pieces.\n",
    "\n",
    "9. Data-Driven Splitting\n",
    "Description: Use machine learning models to determine optimal split points based on document structure. Train a model to identify natural breakpoints in legal documents. Develop and train models using labeled datasets to identify split points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Dense Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ Question #2:\n",
    "What is the embedding dimension, given that we're using text-embedding-3-small?\n",
    "\n",
    "The text-embedding-3-small model from OpenAI has an embedding dimension of 1536 by default. This means that when you use this model to generate embeddings for a piece of text, the resulting embedding vector will have 1536 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Embeddings for Our Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "for chunk in chunks:\n",
    "  embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk - LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n",
      "Chunk - Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
      "\n",
      "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n",
      "Chunk - Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in embeddings_dict.items():\n",
    "  print(f\"Chunk - {k}\")\n",
    "  print(\"---\")\n",
    "  print(f\"Embedding - Vector of Size: {len(v)}\")\n",
    "  print(\"\\n\\n\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector of Size: 1536\n"
     ]
    }
   ],
   "source": [
    "query = \"Can LCEL help take code from the notebook to production?\"\n",
    "\n",
    "query_vector = embedding_model.embed_query(query)\n",
    "print(f\"Vector of Size: {len(query_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec_1, vec_2):\n",
    "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "0.537298487051912\n"
     ]
    }
   ],
   "source": [
    "max_similarity = -float('inf')\n",
    "closest_chunk = \"\"\n",
    "\n",
    "for chunk, chunk_vector in embeddings_dict.items():\n",
    "  cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
    "\n",
    "  if cosine_similarity_score > max_similarity:\n",
    "    closest_chunk = chunk\n",
    "    max_similarity = cosine_similarity_score\n",
    "\n",
    "print(closest_chunk)\n",
    "print(max_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, embeddings_dict, embedding_model):\n",
    "  query_vector = embedding_model.embed_query(query)\n",
    "  max_similarity = -float('inf')\n",
    "  closest_chunk = \"\"\n",
    "\n",
    "  for chunk, chunk_vector in embeddings_dict.items():\n",
    "    cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
    "\n",
    "    if cosine_similarity_score > max_similarity:\n",
    "      closest_chunk = chunk\n",
    "      max_similarity = cosine_similarity_score\n",
    "\n",
    "  return closest_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(query, embeddings_dict, embedding_model, chat_chain):\n",
    "  context = retrieve_context(query, embeddings_dict, embedding_model)\n",
    "\n",
    "  response = chat_chain.invoke({\"query\" : query, \"context\" : context})\n",
    "\n",
    "  return_package = {\n",
    "      \"query\" : query,\n",
    "      \"response\" : response,\n",
    "      \"retriever_context\" : context\n",
    "  }\n",
    "\n",
    "  return return_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Can LCEL help take code from the notebook to production?',\n",
       " 'response': AIMessage(content='Yes, LCEL can help take code from the notebook to production. It allows you to prototype a chain in a Jupyter notebook using the sync interface and then easily expose it as an async streaming interface. This ensures that any chain constructed in this manner has full support for sync, async, batch, and streaming operations, making the transition from prototyping to production smoother.', response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 152, 'total_tokens': 226}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-ffaf24ef-3fc3-44a6-9fc4-11e447089531-0', usage_metadata={'input_tokens': 152, 'output_tokens': 74, 'total_tokens': 226}),\n",
       " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag(\"Can LCEL help take code from the notebook to production?\", embeddings_dict, embedding_model, chat_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ Question #3:\n",
    "What does LCEL do that makes it more reliable at scale?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What does LCEL do that makes it more reliable at scale?',\n",
       " 'response': AIMessage(content='LCEL makes chains more reliable at scale by automatically providing full support for synchronous (sync), asynchronous (async), batch, and streaming interfaces. This ensures that chains can handle various types of workloads and execution models efficiently, making it easier to prototype, scale, and expose them in different ways without needing to rewrite the code for each specific use case.', response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 153, 'total_tokens': 222}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_ce0793330f', 'finish_reason': 'stop', 'logprobs': None}, id='run-718d94e4-2b06-453c-a15f-4a60b612b0e7-0', usage_metadata={'input_tokens': 153, 'output_tokens': 69, 'total_tokens': 222}),\n",
       " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag(\"What does LCEL do that makes it more reliable at scale?\", embeddings_dict, embedding_model, chat_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #5: Create a Simple RAG Application Using Qdrant, OpenAI, and LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Powered RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "docs = PyMuPDFLoader(\"https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf\").load()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Our Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "\n",
    "split_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "max_chunk_length = 0\n",
    "\n",
    "for chunk in split_chunks:\n",
    "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
    "\n",
    "print(max_chunk_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    split_chunks,\n",
    "    embedding_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Steve Job's Speech\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity #2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{question}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       +---------------------------------+                         \n",
      "                       | Parallel<context,question>Input |                         \n",
      "                       +---------------------------------+                         \n",
      "                           *****                   ****                            \n",
      "                        ***                            ****                        \n",
      "                     ***                                   ****                    \n",
      "+--------------------------------+                             **                  \n",
      "| Lambda(itemgetter('question')) |                              *                  \n",
      "+--------------------------------+                              *                  \n",
      "                 *                                              *                  \n",
      "                 *                                              *                  \n",
      "                 *                                              *                  \n",
      "     +----------------------+                   +--------------------------------+ \n",
      "     | VectorStoreRetriever |                   | Lambda(itemgetter('question')) | \n",
      "     +----------------------+                   +--------------------------------+ \n",
      "                           *****                   *****                           \n",
      "                                ***             ***                                \n",
      "                                   ***       ***                                   \n",
      "                       +----------------------------------+                        \n",
      "                       | Parallel<context,question>Output |                        \n",
      "                       +----------------------------------+                        \n",
      "                                         *                                         \n",
      "                                         *                                         \n",
      "                                         *                                         \n",
      "                            +------------------------+                             \n",
      "                            | Parallel<context>Input |                             \n",
      "                            +------------------------+                             \n",
      "                              ****               ****                              \n",
      "                           ***                       ***                           \n",
      "                         **                             **                         \n",
      "     +-------------------------------+              +-------------+                \n",
      "     | Lambda(itemgetter('context')) |              | Passthrough |                \n",
      "     +-------------------------------+              +-------------+                \n",
      "                              ****               ****                              \n",
      "                                  ***         ***                                  \n",
      "                                     **     **                                     \n",
      "                           +-------------------------+                             \n",
      "                           | Parallel<context>Output |                             \n",
      "                           +-------------------------+                             \n",
      "                                         *                                         \n",
      "                                         *                                         \n",
      "                                         *                                         \n",
      "                       +---------------------------------+                         \n",
      "                       | Parallel<response,context>Input |                         \n",
      "                       +---------------------------------+                         \n",
      "                             ***                  ****                             \n",
      "                         ****                         ***                          \n",
      "                       **                                ****                      \n",
      "         +--------------------+                              **                    \n",
      "         | ChatPromptTemplate |                               *                    \n",
      "         +--------------------+                               *                    \n",
      "                    *                                         *                    \n",
      "                    *                                         *                    \n",
      "                    *                                         *                    \n",
      "             +------------+                  +-------------------------------+     \n",
      "             | ChatOpenAI |                  | Lambda(itemgetter('context')) |     \n",
      "             +------------+**                +-------------------------------+     \n",
      "                             ***                  ***                              \n",
      "                                ****          ****                                 \n",
      "                                    **      **                                     \n",
      "                       +----------------------------------+                        \n",
      "                       | Parallel<response,context>Output |                        \n",
      "                       +----------------------------------+                        \n"
     ]
    }
   ],
   "source": [
    "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the most important thing about the iPhone?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most important thing about the iPhone, based on the provided context, is its innovative design and advanced technology. The iPhone integrates a multi-touch screen, miniaturization, custom silicon, power management, OSX inside a mobile device, advanced sensors, desktop-class applications, and a widescreen video iPod. It is designed to be the ultimate digital device, essentially \"like having your life in your pocket.\"'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"response\"].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "page_content='of the art in every facet of this design. So let me just talk a little bit about it here. We’ve got\\nthe multi-touch screen. A first. Miniaturization, more than any we’ve done before. A lot of\\ncustom silicon. Tremendous power management. OSX inside a mobile device. Featherweight\\nprecision enclosures. Three advanced sensors. Desktop class applications, and of course, the\\nwidescreen video iPod. We’ve been innovating like crazy for the last few years on this, and\\nwe filed for over 200 patents for all the inventions in iPhone, and we intend to protect them.\\nSo, a lot of high technology. I think we’re advancing the state of the art in every aspect of\\nthis design. So iPhone is like having your life in your pocket. It’s the ultimate digital device.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 17, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '49b82cac152a402f9e6e3bdce43df0b7', '_collection_name': \"Steve Job's Speech\"}\n",
      "----\n",
      "Context:\n",
      "page_content='what’s on any other phone. Now how do we do this? Well, we start with a strong foundation.\\niPhone runs OSX.\\nNow, why would we want to run such a sophisticated operating system on a mobile\\ndevice? Well, because it’s got everything we need. It’s got multi-tasking. It’s got the best\\nnetworking. It already knows how to power manage. We’ve been doing this on mobile\\ncomputers for years. It’s got awesome security. And the right apps. It’s got everything from\\nCocoa and the graphics and it’s got core animation built in and it’s got the audio and video\\nthat OSX is famous for. It’s got all the stuff we want. And it’s built right in to iPhone. And\\nthat has let us create desktop class applications and networking. Not the crippled stuff that\\nyou find on most phones. This is real, desktop-class applications.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 2, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '808ec23f83d6452c9adc2631964aadda', '_collection_name': \"Steve Job's Speech\"}\n",
      "----\n",
      "Context:\n",
      "page_content='you think. They told me this, they said, You had me at scrolling. So, the iPhone with the\\nmost amazing iPod ever. You can now touch your music. So that’s the iPod.\\nNow, let’s take a look at a revolutionary phone. We want to reinvent the phone. Now, what’s\\nthe killer app? The killer app is making calls! It’s amazing — it’s amazing how hard it is to\\nmake calls on most phones. Most people actually dial them every time. Most people don’t\\nhave very many numbers in their address book they use their recents as their address book.\\nRight? How many of you do that? I bet more than a few. So, we want to let you use contacts\\nlike never before. You can synch your iPhone with your PC or Mac and bring down all your\\ncontacts right into your phone. So you’ve got everybody’s numbers with you at all times.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 6, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '2a0c6f1e2c3c424bb6cca2e60443208a', '_collection_name': \"Steve Job's Speech\"}\n",
      "----\n",
      "Context:\n",
      "page_content='you set up what you want synched to your iPhone. And it’s just like an iPod. Charge and\\nsynch. So synch with iTunes.\\nThird thing I want to talk about a little is design. We’ve designed something wonderful for\\nyour hand, just wonderful. This is what it looks like. It’s got a three-and-a-half-inch screen\\non it. It’s really big. And, it’s the highest-resolution screen we’ve ever shipped. It’s 160\\npixels per inch. Highest we’ve ever shipped. It’s gorgeous. And on the front, there’s only one\\nbutton down there. We call it the Home button. Takes you Home from wherever you are.\\nAnd that’s it.\\nLet’s take a look at the side. It’s really thin. It’s thinner than any smartphone out there, at' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 3, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '8b7820da18ce4b5fa86455ecbcca50d8', '_collection_name': \"Steve Job's Speech\"}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for context in response[\"context\"]:\n",
    "  print(\"Context:\")\n",
    "  print(context)\n",
    "  print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"response\"].content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ Question #4:\n",
    "What key innovations did the iPhone introduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The key innovations introduced by the iPhone, as highlighted in the provided context, include:\\n\\n1. **Widescreen iPod with Touch Controls**: This innovation allowed users to touch their music and interact with it in a more intuitive way.\\n\\n2. **Revolutionary Mobile Phone**: The iPhone aimed to reinvent the phone by making it easier to make calls and manage contacts. It allowed users to sync their contacts from their PC or Mac to their phone, ensuring they had all their numbers with them at all times.\\n\\n3. **Breakthrough Internet Communications Device**: The iPhone combined an iPod, a phone, and an Internet communicator into one device, changing the way users interacted with the internet on a mobile device.\\n\\n4. **Multi-touch Screen**: This was a first in mobile devices, allowing for more advanced and intuitive touch interactions.\\n\\n5. **Miniaturization and Custom Silicon**: The iPhone featured significant advancements in miniaturization and custom silicon design for better performance and power management.\\n\\n6. **OSX Inside a Mobile Device**: The inclusion of OSX provided desktop-class applications on a mobile device.\\n\\n7. **Advanced Sensors**: The iPhone included three advanced sensors to enhance usability and functionality.\\n\\n8. **Featherweight Precision Enclosures**: The design emphasized lightweight and precise construction.\\n\\n9. **Widescreen Video iPod**: It featured a widescreen display for video playback, enhancing the media consumption experience.\\n\\nOverall, the iPhone integrated multiple high-tech features and aimed to advance the state of the art in mobile device design.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What key innovations did the iPhone introduce?\"})\n",
    "response[\"response\"].content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
