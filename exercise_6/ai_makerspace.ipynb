{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain v0.2.0 and LCEL: LangChain Powered RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder you are executing pip from can no longer be found.\n",
      "The folder you are executing pip from can no longer be found.\n",
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain langchain-core langchain-community langchain-openai\n",
    "!pip install -qU qdrant-client\n",
    "!pip install -qU tiktoken pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Initialize a Simple Chain using LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Orchestration Tool (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Question #1:\n",
    "What other models could we use, and how would the above code change?\n",
    "\n",
    "1. gpt-3.5-turbo\n",
    "2. gpt-4\n",
    "3. gpt-4-domain-specific\n",
    "4. babbage-002\n",
    "5. davinci-002\n",
    "\n",
    "We need to change the value of model paramter to the name of the model in order to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"You are a legendary and mythical Wizard. You speak in riddles and make obscure and pun-filled references to exotic cheeses.\"\n",
    "human_template = \"{content}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our First Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | openai_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ah, greetings, seeker of wisdom! Just as a wheel of Gouda rolls through the valleys of time, your words echo through the ages. What curdled curiosity brings you to my enchanted realm today?' response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 38, 'total_tokens': 80}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4008e3b719', 'finish_reason': 'stop', 'logprobs': None} id='run-6bf42ed0-85f9-49c3-93e3-dd98e428b3eb-0' usage_metadata={'input_tokens': 38, 'output_tokens': 42, 'total_tokens': 80}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"content\": \"Hello world!\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, seeker of serpentine wisdom, to master the Python’s embrace, one must dance with the code like a Roquefort in a river of milk. Heed these cryptic morsels of counsel:\\n\\n1. **Gaze into the Gouda**: Study the ancient scripts and scrolls, known to mere mortals as documentation. The Python Software Foundation’s tomes and texts are your sacred cheese wedges.\\n\\n2. **Mingle with the Muenster**: Join the circles of fellow enchanters in places like GitHub, Stack Overflow, or the Python Discord. Their shared spells and incantations shall be your guiding light.\\n\\n3. **Slice the Swiss**: Write code with holes, yet refine it until it becomes a solid block. Practice by tackling small projects, then gradually gnaw your way to grander wheels.\\n\\n4. **Taste the Taleggio**: Savor the art of testing. Ensure your code is as smooth and consistent as a well-aged cheese. Employ tools like pytest to certify your concoctions.\\n\\n5. **Revere the Ricotta**: Make your code as fresh and simple as a soft cheese. Follow the Zen of Python, those 19 aphorisms that are the curds and whey of wise programming.\\n\\n6. **Grate the Gruyère**: Refactor your code to remove the excess rind, making it efficient and delectable. Seek out duplicate patterns and slice them away.\\n\\n7. **Bask in the Brie**: Engage in code reviews, both giving and receiving. The collective wisdom will melt away your misunderstandings and solidify your strengths.\\n\\n8. **Nibble on the Neufchâtel**: Explore the diverse libraries and frameworks that Python offers, like Django, Flask, or TensorFlow. Each one adds a unique flavor to your repertoire.\\n\\n9. **Embrace the Emmental**: As you grow, delve into the depths of advanced topics such as concurrency, design patterns, and performance optimization. These are the complex cheeses that will set you apart.\\n\\n10. **Quaff the Queso**: Regularly partake in the rituals of coding challenges and hackathons. These crucibles of creativity shall sharpen your skills and broaden your horizons.\\n\\nRemember, dear apprentice, the journey to Python mastery is a feast, not a sprint. Savor each byte, and let the flavors of knowledge marinate within you.', response_metadata={'token_usage': {'completion_tokens': 486, 'prompt_tokens': 50, 'total_tokens': 536}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4008e3b719', 'finish_reason': 'stop', 'logprobs': None}, id='run-b1554202-a9de-4ba1-888f-b138e03bd7e4-0', usage_metadata={'input_tokens': 50, 'output_tokens': 486, 'total_tokens': 536})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"content\" : \"Could I please have some advice on how to become a better Python Programmer?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG - Manually adding context through the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides a structured approach to building language model-powered applications by linking various components and capabilities. The core functionalities of LangChain include:\\n\\n1. **Prompt Management**: Tools for creating, formatting, and managing prompts for language models.\\n2. **LLM Chaining**: Methods to chain together multiple calls to language models, enabling more complex interactions and workflows.\\n3. **Data Augmented Generation**: Techniques to incorporate external data sources to enhance the responses generated by language models.\\n4. **Agents**: Components that allow language models to interact with other tools and services, effectively acting as intelligent agents.\\n5. **Memory**: Mechanisms to maintain state and context across interactions, which is crucial for applications requiring ongoing dialogues or sessions.\\n\\nBy integrating these capabilities, LangChain aims to simplify the process of building robust, feature-rich applications that leverage the power of large language models.' response_metadata={'token_usage': {'completion_tokens': 197, 'prompt_tokens': 22, 'total_tokens': 219}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None} id='run-f029f8bd-b09a-45cd-94d9-da4722f7fe19-0' usage_metadata={'input_tokens': 22, 'output_tokens': 197, 'total_tokens': 219}\n"
     ]
    }
   ],
   "source": [
    "system_template = \"You are a helpful assistant.\"\n",
    "human_template = \"{content}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | openai_chat_model\n",
    "\n",
    "print(chat_chain.invoke({\"content\" : \"Please define LangChain.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain Expression Language (LEL) is a specialized language designed for the LangChain framework, which is used to manage and manipulate chains of data processing tasks. LEL enables developers to define, configure, and execute complex workflows involving multiple data processing steps in a declarative manner.\\n\\nBy using LEL, developers can:\\n\\n1. **Define Chains:** Specify a sequence of operations or tasks that data should pass through.\\n2. **Configure Parameters:** Set parameters and configurations for each task in the chain.\\n3. **Manage Dependencies:** Handle dependencies and data flow between different tasks.\\n4. **Optimize Performance:** Optimize the execution of tasks by managing resources and parallelism.\\n\\nLEL abstracts the complexity involved in orchestrating multiple data processing tasks, making it easier for developers to build and maintain scalable data pipelines. It is typically used in environments where data needs to be processed through a series of transformations, aggregations, or other operations in a structured and repeatable way.' response_metadata={'token_usage': {'completion_tokens': 192, 'prompt_tokens': 27, 'total_tokens': 219}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None} id='run-c09c4809-cd26-4652-b7de-6d47dc77243c-0' usage_metadata={'input_tokens': 27, 'output_tokens': 192, 'total_tokens': 219}\n"
     ]
    }
   ],
   "source": [
    "print(chat_chain.invoke({\"content\" : \"What is LangChain Expression Language (LECL)?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It provides several benefits, such as full sync, async, batch, and streaming support, the ability to attach fallbacks to handle errors gracefully, automatic parallel execution of components that can be run in parallel, and seamless integration with LangSmith for tracing and observability.' response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 274, 'total_tokens': 346}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None} id='run-1f377483-6d8b-45fb-98db-ba80a4fcff09-0' usage_metadata={'input_tokens': 274, 'output_tokens': 72, 'total_tokens': 346}\n"
     ]
    }
   ],
   "source": [
    "HUMAN_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT = \"\"\"\n",
    "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
    "\n",
    "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
    "\n",
    "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
    "\n",
    "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "\n",
    "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | openai_chat_model\n",
    "\n",
    "print(chat_chain.invoke({\"query\" : \"What is LangChain Expression Language?\", \"context\" : CONTEXT}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #4: Implement Naive RAG using LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the R in RAG: Retrieval 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "EVERY HITCHHIKER'S GUIDE BOOK\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc.encode(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextSplitting aka Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(CONTEXT)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "----\n",
      "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
      "\n",
      "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
      "----\n",
      "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "  print(chunk)\n",
    "  print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity #1:\n",
    "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
    "\n",
    "Brainstorm some ideas that would split large single documents into smaller documents.\n",
    "--------------------------------------------------------\n",
    "Splitting large single documents into smaller, more manageable pieces can be approached in various ways, depending on the type of document and the intended use. Here are some ideas:\n",
    "\n",
    "1. Section-Based Splitting\n",
    "Use a parser to identify headings and split the text accordingly.\n",
    "\n",
    "2. Paragraph-Based Splitting\n",
    "Identify paragraph breaks and split the document at these points.\n",
    "\n",
    "3. Sentence-Based Splitting\n",
    "Use natural language processing (NLP) tools to detect sentence boundaries.\n",
    "\n",
    "4. Topic-Based Splitting\n",
    "Apply algorithms like Latent Dirichlet Allocation (LDA) to identify topics and split the document.\n",
    "\n",
    "5. Page-Based Splitting\n",
    "Use PDF manipulation tools to split the document by pages.\n",
    "\n",
    "6. Keyword-Based Splitting\n",
    "Use regular expressions or NLP to detect keywords and split the text.\n",
    "\n",
    "7. Time-Based Splitting (for transcripts)\n",
    "Split audio or video transcripts into smaller sections based on time intervals. Use the timestamps in the transcript to guide the splitting process.\n",
    "\n",
    "8. Summarization-Based Splitting\n",
    "Generate summaries of smaller sections from the large document. Use text summarization algorithms to condense the content into smaller pieces.\n",
    "\n",
    "9. Data-Driven Splitting\n",
    "Description: Use machine learning models to determine optimal split points based on document structure. Train a model to identify natural breakpoints in legal documents. Develop and train models using labeled datasets to identify split points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Dense Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ Question #2:\n",
    "What is the embedding dimension, given that we're using text-embedding-3-small?\n",
    "\n",
    "The text-embedding-3-small model from OpenAI has an embedding dimension of 1536 by default. This means that when you use this model to generate embeddings for a piece of text, the resulting embedding vector will have 1536 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Embeddings for Our Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "for chunk in chunks:\n",
    "  embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk - LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n",
      "Chunk - Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
      "\n",
      "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n",
      "Chunk - Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in embeddings_dict.items():\n",
    "  print(f\"Chunk - {k}\")\n",
    "  print(\"---\")\n",
    "  print(f\"Embedding - Vector of Size: {len(v)}\")\n",
    "  print(\"\\n\\n\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector of Size: 1536\n"
     ]
    }
   ],
   "source": [
    "query = \"Can LCEL help take code from the notebook to production?\"\n",
    "\n",
    "query_vector = embedding_model.embed_query(query)\n",
    "print(f\"Vector of Size: {len(query_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec_1, vec_2):\n",
    "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "0.537298487051912\n"
     ]
    }
   ],
   "source": [
    "max_similarity = -float('inf')\n",
    "closest_chunk = \"\"\n",
    "\n",
    "for chunk, chunk_vector in embeddings_dict.items():\n",
    "  cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
    "\n",
    "  if cosine_similarity_score > max_similarity:\n",
    "    closest_chunk = chunk\n",
    "    max_similarity = cosine_similarity_score\n",
    "\n",
    "print(closest_chunk)\n",
    "print(max_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, embeddings_dict, embedding_model):\n",
    "  query_vector = embedding_model.embed_query(query)\n",
    "  max_similarity = -float('inf')\n",
    "  closest_chunk = \"\"\n",
    "\n",
    "  for chunk, chunk_vector in embeddings_dict.items():\n",
    "    cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
    "\n",
    "    if cosine_similarity_score > max_similarity:\n",
    "      closest_chunk = chunk\n",
    "      max_similarity = cosine_similarity_score\n",
    "\n",
    "  return closest_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(query, embeddings_dict, embedding_model, chat_chain):\n",
    "  context = retrieve_context(query, embeddings_dict, embedding_model)\n",
    "\n",
    "  response = chat_chain.invoke({\"query\" : query, \"context\" : context})\n",
    "\n",
    "  return_package = {\n",
    "      \"query\" : query,\n",
    "      \"response\" : response,\n",
    "      \"retriever_context\" : context\n",
    "  }\n",
    "\n",
    "  return return_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Can LCEL help take code from the notebook to production?',\n",
       " 'response': AIMessage(content='Yes, LCEL can help take code from the notebook to production. By writing chains in LCEL, you can easily prototype a chain in a Jupyter notebook using the sync interface and then expose it as an async streaming interface. This provides a seamless transition from development in a notebook environment to production-ready code with full sync, async, batch, and streaming support.', response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 152, 'total_tokens': 225}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-b9e3552e-0afd-436c-8fb8-f2ec361a8065-0', usage_metadata={'input_tokens': 152, 'output_tokens': 73, 'total_tokens': 225}),\n",
       " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag(\"Can LCEL help take code from the notebook to production?\", embeddings_dict, embedding_model, chat_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ Question #3:\n",
    "What does LCEL do that makes it more reliable at scale?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What does LCEL do that makes it more reliable at scale?',\n",
       " 'response': AIMessage(content='LCEL makes chains more reliable at scale by providing full support for synchronous, asynchronous, batch, and streaming operations. This flexibility allows for easier prototyping and ensures that the chains can handle various types of workloads efficiently, making them more robust and scalable.', response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 153, 'total_tokens': 203}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-33b43168-0dbf-45c6-8ebe-e886c9b26f7b-0', usage_metadata={'input_tokens': 153, 'output_tokens': 50, 'total_tokens': 203}),\n",
       " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag(\"What does LCEL do that makes it more reliable at scale?\", embeddings_dict, embedding_model, chat_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #5: Create a Simple RAG Application Using Qdrant, OpenAI, and LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Powered RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "docs = PyMuPDFLoader(\"https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf\").load()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Our Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "\n",
    "split_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "max_chunk_length = 0\n",
    "\n",
    "for chunk in split_chunks:\n",
    "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
    "\n",
    "print(max_chunk_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    split_chunks,\n",
    "    embedding_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Steve Job's Speech\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity #2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{question}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       +---------------------------------+                         \n",
      "                       | Parallel<context,question>Input |                         \n",
      "                       +---------------------------------+                         \n",
      "                           *****                   ****                            \n",
      "                        ***                            ****                        \n",
      "                     ***                                   ****                    \n",
      "+--------------------------------+                             **                  \n",
      "| Lambda(itemgetter('question')) |                              *                  \n",
      "+--------------------------------+                              *                  \n",
      "                 *                                              *                  \n",
      "                 *                                              *                  \n",
      "                 *                                              *                  \n",
      "     +----------------------+                   +--------------------------------+ \n",
      "     | VectorStoreRetriever |                   | Lambda(itemgetter('question')) | \n",
      "     +----------------------+                   +--------------------------------+ \n",
      "                           *****                   *****                           \n",
      "                                ***             ***                                \n",
      "                                   ***       ***                                   \n",
      "                       +----------------------------------+                        \n",
      "                       | Parallel<context,question>Output |                        \n",
      "                       +----------------------------------+                        \n",
      "                                         *                                         \n",
      "                                         *                                         \n",
      "                                         *                                         \n",
      "                            +------------------------+                             \n",
      "                            | Parallel<context>Input |                             \n",
      "                            +------------------------+                             \n",
      "                              ****               ****                              \n",
      "                           ***                       ***                           \n",
      "                         **                             **                         \n",
      "     +-------------------------------+              +-------------+                \n",
      "     | Lambda(itemgetter('context')) |              | Passthrough |                \n",
      "     +-------------------------------+              +-------------+                \n",
      "                              ****               ****                              \n",
      "                                  ***         ***                                  \n",
      "                                     **     **                                     \n",
      "                           +-------------------------+                             \n",
      "                           | Parallel<context>Output |                             \n",
      "                           +-------------------------+                             \n",
      "                                         *                                         \n",
      "                                         *                                         \n",
      "                                         *                                         \n",
      "                       +---------------------------------+                         \n",
      "                       | Parallel<response,context>Input |                         \n",
      "                       +---------------------------------+                         \n",
      "                             ***                  ****                             \n",
      "                         ****                         ***                          \n",
      "                       **                                ****                      \n",
      "         +--------------------+                              **                    \n",
      "         | ChatPromptTemplate |                               *                    \n",
      "         +--------------------+                               *                    \n",
      "                    *                                         *                    \n",
      "                    *                                         *                    \n",
      "                    *                                         *                    \n",
      "             +------------+                  +-------------------------------+     \n",
      "             | ChatOpenAI |                  | Lambda(itemgetter('context')) |     \n",
      "             +------------+**                +-------------------------------+     \n",
      "                             ***                  ***                              \n",
      "                                ****          ****                                 \n",
      "                                    **      **                                     \n",
      "                       +----------------------------------+                        \n",
      "                       | Parallel<response,context>Output |                        \n",
      "                       +----------------------------------+                        \n"
     ]
    }
   ],
   "source": [
    "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the most important thing about the iPhone?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most important thing about the iPhone, as highlighted in the provided context, is its comprehensive and advanced design that integrates various high technologies. This includes features such as a multi-touch screen, miniaturization, custom silicon, power management, OSX inside a mobile device, advanced sensors, desktop-class applications, and its function as a widescreen video iPod. The iPhone is described as the \"ultimate digital device\" that can hold your life in your pocket, representing significant innovation in mobile technology.\\n\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"response\"].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "page_content='of the art in every facet of this design. So let me just talk a little bit about it here. We’ve got\\nthe multi-touch screen. A first. Miniaturization, more than any we’ve done before. A lot of\\ncustom silicon. Tremendous power management. OSX inside a mobile device. Featherweight\\nprecision enclosures. Three advanced sensors. Desktop class applications, and of course, the\\nwidescreen video iPod. We’ve been innovating like crazy for the last few years on this, and\\nwe filed for over 200 patents for all the inventions in iPhone, and we intend to protect them.\\nSo, a lot of high technology. I think we’re advancing the state of the art in every aspect of\\nthis design. So iPhone is like having your life in your pocket. It’s the ultimate digital device.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 17, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': 'fe212d25472b4dc5bd2a32f74041e051', '_collection_name': \"Steve Job's Speech\"}\n",
      "----\n",
      "Context:\n",
      "page_content='what’s on any other phone. Now how do we do this? Well, we start with a strong foundation.\\niPhone runs OSX.\\nNow, why would we want to run such a sophisticated operating system on a mobile\\ndevice? Well, because it’s got everything we need. It’s got multi-tasking. It’s got the best\\nnetworking. It already knows how to power manage. We’ve been doing this on mobile\\ncomputers for years. It’s got awesome security. And the right apps. It’s got everything from\\nCocoa and the graphics and it’s got core animation built in and it’s got the audio and video\\nthat OSX is famous for. It’s got all the stuff we want. And it’s built right in to iPhone. And\\nthat has let us create desktop class applications and networking. Not the crippled stuff that\\nyou find on most phones. This is real, desktop-class applications.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 2, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '5d7d235bef8b4f9ca0d0ae90b1fc2d85', '_collection_name': \"Steve Job's Speech\"}\n",
      "----\n",
      "Context:\n",
      "page_content='you think. They told me this, they said, You had me at scrolling. So, the iPhone with the\\nmost amazing iPod ever. You can now touch your music. So that’s the iPod.\\nNow, let’s take a look at a revolutionary phone. We want to reinvent the phone. Now, what’s\\nthe killer app? The killer app is making calls! It’s amazing — it’s amazing how hard it is to\\nmake calls on most phones. Most people actually dial them every time. Most people don’t\\nhave very many numbers in their address book they use their recents as their address book.\\nRight? How many of you do that? I bet more than a few. So, we want to let you use contacts\\nlike never before. You can synch your iPhone with your PC or Mac and bring down all your\\ncontacts right into your phone. So you’ve got everybody’s numbers with you at all times.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 6, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': 'f2331646400741e99185f36164e8b74a', '_collection_name': \"Steve Job's Speech\"}\n",
      "----\n",
      "Context:\n",
      "page_content='you set up what you want synched to your iPhone. And it’s just like an iPod. Charge and\\nsynch. So synch with iTunes.\\nThird thing I want to talk about a little is design. We’ve designed something wonderful for\\nyour hand, just wonderful. This is what it looks like. It’s got a three-and-a-half-inch screen\\non it. It’s really big. And, it’s the highest-resolution screen we’ve ever shipped. It’s 160\\npixels per inch. Highest we’ve ever shipped. It’s gorgeous. And on the front, there’s only one\\nbutton down there. We call it the Home button. Takes you Home from wherever you are.\\nAnd that’s it.\\nLet’s take a look at the side. It’s really thin. It’s thinner than any smartphone out there, at' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 3, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': 'e33b178cdfe84ecb8d13afb179c9378c', '_collection_name': \"Steve Job's Speech\"}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for context in response[\"context\"]:\n",
    "  print(\"Context:\")\n",
    "  print(context)\n",
    "  print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"response\"].content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ Question #4:\n",
    "What key innovations did the iPhone introduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The iPhone introduced several key innovations according to the provided context:\\n\\n1. **Combination of Multiple Devices**: It combined three devices into one — a widescreen iPod with touch controls, a revolutionary mobile phone, and a breakthrough Internet communications device.\\n   \\n2. **Touch Controls**: It featured touch controls that allowed users to interact with their music in new ways.\\n\\n3. **Ease of Making Calls**: It simplified making calls by allowing users to sync their contacts from their PC or Mac, making it easier to access and use contact information.\\n\\n4. **Multi-Touch Screen**: It included a multi-touch screen, which was a first in mobile devices.\\n\\n5. **Miniaturization and Custom Silicon**: The iPhone featured advanced miniaturization and custom silicon for better performance and power management.\\n\\n6. **OSX in a Mobile Device**: It incorporated OSX, providing a powerful operating system in a mobile device.\\n\\n7. **Advanced Sensors**: The iPhone had three advanced sensors for improved functionality.\\n\\n8. **Desktop-Class Applications**: It supported desktop-class applications, enhancing its utility and performance.\\n\\n9. **Widescreen Video iPod**: It functioned as a widescreen video iPod, allowing for better media consumption.\\n\\n10. **Extensive Patents**: Apple filed for over 200 patents for the various innovations in the iPhone, intending to protect these technological advancements. \\n\\nIn summary, the iPhone was described as the \"ultimate digital device\" that advanced the state of the art in mobile technology.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What key innovations did the iPhone introduce?\"})\n",
    "response[\"response\"].content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
