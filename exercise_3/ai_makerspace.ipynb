{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install -qU numpy matplotlib plotly pandas scipy scikit-learn openai python-dotenv pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
    "from aimakerspace.vectordatabase import VectorDatabase, cosine_similarity\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Source Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>> data/machine-learning-yearning.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_loader = TextFileLoader(\"data/machine-learning-yearning.pdf\")\n",
    "documents = text_loader.load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Machine Learning Yearning is a\\ndeeplearning.ai project.\\n© 2018 Andrew Ng. All Rights Reserved.\\nPage 2 Machine Learning Yearning-Draft Andrew Ng Table of Contents\\n1 Why Machine Learning Strategy\\n2 How to use this book to help your team\\n3 Prerequisites and Notation\\n4 Scale drives machine learning progress\\n5 Your development and test sets\\n6 Your dev and test sets should come from the same distribution\\n7 How large do the dev/test sets need to be?\\n8 Establish a single-number evaluation metric for your team to optimize\\n9 Optimizing and satisficing metrics\\n10 Having a dev set and metric speeds up iterations\\n11 When to change dev/test sets and metrics\\n12 Takeaways: Setting up development and test sets\\n13 Build your first system quickly, then iterate\\n14 Error analysis: Look at dev set examples to evaluate ideas\\n15 Evaluating multiple ideas in parallel during error analysis\\n16 Cleaning up mislabeled dev and test set examples\\n17 If you have a large dev set, split it into two subsets, only one o']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_documents[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Embeddings and Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8339742745295414"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
    "embedding_model = EmbeddingModel()\n",
    "puppy_sentence = \"I love puppies!\"\n",
    "dog_sentence = \"I love dogs!\"\n",
    "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
    "dog_vector = embedding_model.get_embedding(dog_sentence)\n",
    "cosine_similarity(puppy_vector, dog_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3725204049494281"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puppy_sentence = \"I love puppies!\"\n",
    "cat_sentence = \"I dislike cats!\"\n",
    "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
    "cat_vector = embedding_model.get_embedding(cat_sentence)\n",
    "\n",
    "cosine_similarity(puppy_vector, cat_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1:\n",
    "The default embedding dimension of text-embedding-3-small is 1536, as noted above.\n",
    "\n",
    "1. Is there any way to modify this dimension? Yes pass `dimensions` parameter with the required value to modify default embedding dimension\n",
    "2. What technique does OpenAI use to achieve this? OpenAI does it through `Matryoshka Representation Learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = VectorDatabase()\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2:\n",
    "What are the benefits of using an async approach to collecting our embeddings?\n",
    "\n",
    "Benefits of using an async approach is performance. Performance can be improved for a code that is IO bound, when the system is waiting for a response for one request, it can process something else in a meanwhile (prepare and send all other request in our case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nts:\\n1. Parser: A system that annotates the text with information identifying the most\\n15\\nimportant words. For example, you might use the parser to label all the adjectives\\nand nouns. You would therefore get the following annotated text:\\nThis is a great mop !\\n\\u200bAdjective \\u200bNoun\\n\\u200b \\u200b\\n2. Sentiment classifier: A learning algorithm that takes as input the annotated text and\\npredicts the overall sentiment. The parser’s annotation could help this learning\\nalgorithm greatly: By giving adjectives a higher weight, your algorithm will be able to\\nquickly hone in on the important words such as “great,” and ignore less important\\nwords such as “this.”\\nWe can visualize your “pipeline” of two components as follows:\\nThere has been a recent trend toward replacing pipeline systems with a single learning\\nalgorithm. An end-to-end learning algorithm for this task would simply take as input\\n\\u200b \\u200b\\nthe raw, original text “This is a great mop!”, and try to directly recognize the sentiment:\\n15 A parser gives a much r',\n",
       "  1.4009156833211214),\n",
       " ('ou can download this\\ndata from several websites. Suppose you also have a large training set of people speaking in a\\nquiet room. If you take an audio clip of a person speaking and “add” to that to an audio clip\\nof car/road noise, you will obtain an audio clip that sounds as if that person was speaking in\\na noisy car. Using this process, you can “synthesize” huge amounts of data that sound as if it\\nwere collected inside a car.\\nMore generally, there are several circumstances where artificial data synthesis allows you to\\ncreate a huge dataset that reasonably matches the dev set. Let’s use the cat image detector as\\na second example. You notice that dev set images have much more motion blur because they\\ntend to come from cellphone users who are moving their phone slightly while taking the\\npicture. You can take non-blurry images from the training set of internet images, and add\\nsimulated motion blur to them, thus making them more similar to the dev set.\\nKeep in mind that artificial data synth',\n",
       "  1.391597768438065),\n",
       " ('rther, each of\\nthese is a relatively simpler function--and can thus be learned with less data--than the\\npurely end-to-end approach.\\nIn summary, when deciding what should be the components of a pipeline, try to build a\\npipeline where each component is a relatively “simple” function that can therefore be learned\\nfrom only a modest amount of data.\\nPage 102 Machine Learning Yearning-Draft Andrew Ng 52 Directly learning rich outputs\\nAn image classification algorithm will input an image x, and output an integer indicating the\\n\\u200b \\u200b\\nobject category. Can an algorithm instead output an entire sentence describing the image?\\nFor example:\\nx = y = “A yellow bus driving down a road with\\n\\u200b \\u200b\\ngreen trees and green grass in the\\nbackground.”\\nTraditional applications of supervised learning learned a function h:X→\\u200bY, where the output\\n\\u200b \\u200b \\u200b \\u200b \\u200b\\ny was usually an integer or a real number. For example:\\n\\u200b\\nProblem X Y\\nSpam classification Email Spam/Not spam (0/1)\\nImage recognition Image Integer label\\nHousing pric',\n",
       "  1.3780227317960563)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.search_by_text(\"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatOpenAI\n",
    "## Question #3:\n",
    "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
    "Yes, set the seed parameter to any integer of your choice and use the same value across requests you'd like deterministic outputs for.\n",
    "Ensure all other parameters (like prompt or temperature) are the exact same across requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Prompting OpenAI's gpt-3.5-turbo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.openai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    "    AssistantRolePrompt,\n",
    ")\n",
    "\n",
    "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
    "\n",
    "chat_openai = ChatOpenAI()\n",
    "user_prompt_template = \"{content}\"\n",
    "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
    "system_prompt_template = (\n",
    "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
    ")\n",
    "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
    "\n",
    "messages = [\n",
    "    user_role_prompt.create_message(\n",
    "        content=\"What is the best way to write a loop?\"\n",
    "    ),\n",
    "    system_role_prompt.create_message(expertise=\"Python\"),\n",
    "]\n",
    "response = chat_openai.run(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to write a loop ultimately depends on the specific requirements of your program and personal preference. However, in general, using the most appropriate looping construct for the task at hand is key. \n",
      "\n",
      "For simple iteration over a range of values, a `for` loop is often the easiest and most readable choice. For example:\n",
      "\n",
      "```python\n",
      "for i in range(5):\n",
      "    print(i)\n",
      "```\n",
      "\n",
      "If you need to iterate based on a condition, a `while` loop might be more appropriate:\n",
      "\n",
      "```python\n",
      "n = 0\n",
      "while n < 5:\n",
      "    print(n)\n",
      "    n += 1\n",
      "```\n",
      "\n",
      "Remember to ensure your loop has a proper exit condition to prevent infinite loops. Additionally, using clear and descriptive variable names, as well as commenting your code, can help improve readability and maintainability.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
    "Use the provided context to answer the user's query.\n",
    "\n",
    "You may not answer the user's query unless there is specific context in the following text.\n",
    "\n",
    "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
    "\n",
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "\n",
    "    def run_pipeline(self, user_query: str) -> str:\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
    "\n",
    "        context_prompt = \"\"\n",
    "        for context in context_list:\n",
    "            context_prompt += context[0] + \"\\n\"\n",
    "\n",
    "        formatted_system_prompt = rag_prompt.create_message()\n",
    "\n",
    "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
    "\n",
    "        return {\"response\" : self.llm.run([formatted_user_prompt, formatted_system_prompt]), \"context\" : context_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #4:\n",
    "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
    "\n",
    "What is that strategy called?\n",
    "It is called `Chain of Thought Prompting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_augmented_qa_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"I don't know.\",\n",
       " 'context': [('nts:\\n1. Parser: A system that annotates the text with information identifying the most\\n15\\nimportant words. For example, you might use the parser to label all the adjectives\\nand nouns. You would therefore get the following annotated text:\\nThis is a great mop !\\n\\u200bAdjective \\u200bNoun\\n\\u200b \\u200b\\n2. Sentiment classifier: A learning algorithm that takes as input the annotated text and\\npredicts the overall sentiment. The parser’s annotation could help this learning\\nalgorithm greatly: By giving adjectives a higher weight, your algorithm will be able to\\nquickly hone in on the important words such as “great,” and ignore less important\\nwords such as “this.”\\nWe can visualize your “pipeline” of two components as follows:\\nThere has been a recent trend toward replacing pipeline systems with a single learning\\nalgorithm. An end-to-end learning algorithm for this task would simply take as input\\n\\u200b \\u200b\\nthe raw, original text “This is a great mop!”, and try to directly recognize the sentiment:\\n15 A parser gives a much r',\n",
       "   1.4018009814477954),\n",
       "  ('ou can download this\\ndata from several websites. Suppose you also have a large training set of people speaking in a\\nquiet room. If you take an audio clip of a person speaking and “add” to that to an audio clip\\nof car/road noise, you will obtain an audio clip that sounds as if that person was speaking in\\na noisy car. Using this process, you can “synthesize” huge amounts of data that sound as if it\\nwere collected inside a car.\\nMore generally, there are several circumstances where artificial data synthesis allows you to\\ncreate a huge dataset that reasonably matches the dev set. Let’s use the cat image detector as\\na second example. You notice that dev set images have much more motion blur because they\\ntend to come from cellphone users who are moving their phone slightly while taking the\\npicture. You can take non-blurry images from the training set of internet images, and add\\nsimulated motion blur to them, thus making them more similar to the dev set.\\nKeep in mind that artificial data synth',\n",
       "   1.387811844441637),\n",
       "  ('rther, each of\\nthese is a relatively simpler function--and can thus be learned with less data--than the\\npurely end-to-end approach.\\nIn summary, when deciding what should be the components of a pipeline, try to build a\\npipeline where each component is a relatively “simple” function that can therefore be learned\\nfrom only a modest amount of data.\\nPage 102 Machine Learning Yearning-Draft Andrew Ng 52 Directly learning rich outputs\\nAn image classification algorithm will input an image x, and output an integer indicating the\\n\\u200b \\u200b\\nobject category. Can an algorithm instead output an entire sentence describing the image?\\nFor example:\\nx = y = “A yellow bus driving down a road with\\n\\u200b \\u200b\\ngreen trees and green grass in the\\nbackground.”\\nTraditional applications of supervised learning learned a function h:X→\\u200bY, where the output\\n\\u200b \\u200b \\u200b \\u200b \\u200b\\ny was usually an integer or a real number. For example:\\n\\u200b\\nProblem X Y\\nSpam classification Email Spam/Not spam (0/1)\\nImage recognition Image Integer label\\nHousing pric',\n",
       "   1.3766337440948075),\n",
       "  (' cat breed classifier--seem much easier\\nto learn and will require significantly less data.17\\n17 If you are familiar with practical object detection algorithms, you will recognize that they do not learn just with 0/1\\nimage labels, but are instead trained with bounding boxes provided as part of the training data. A discussion of them is\\nbeyond the scope of this chapter. See the Deep Learning specialization on Coursera (http://deeplearning.ai) if you would\\n\\u200b \\u200b\\nlike to learn more about such algorithms.\\nPage 101 Machine Learning Yearning-Draft Andrew Ng As one final example, let’s revisit the autonomous driving pipeline.\\nBy using this pipeline, you are telling the algorithm that there are 3 key steps to driving: (1)\\nDetect other cars, (2) Detect pedestrians, and (3) Plan a path for your car. Further, each of\\nthese is a relatively simpler function--and can thus be learned with less data--than the\\npurely end-to-end approach.\\nIn summary, when deciding what should be the components of a pipelin',\n",
       "   1.3753677150715549)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_augmented_qa_pipeline.run_pipeline(\"What is the 'Michael Eisner Memorial Weak Executive Problem'?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"I don't see specific information in the provided text regarding how to clean up mislabeled dev and test set examples.\",\n",
       " 'context': [(', then perhaps the reward is R(T) = -1,000—a huge negative reward. A\\n\\u200b \\u200b\\ntrajectory T resulting in a safe landing might result in a positive R(T) with the exact value\\n\\u200b \\u200b \\u200b \\u200b\\ndepending on how smooth the landing was. The reward function R(.) is typically chosen by\\n\\u200b \\u200b\\nhand to quantify how desirable different trajectories T are. It has to trade off how bumpy the\\n\\u200b \\u200b\\nlanding was, whether the helicopter landed in exactly the desired spot, how rough the ride\\ndown was for passengers, and so on. It is not easy to design good reward functions.\\nPage 88 Machine Learning Yearning-Draft Andrew Ng Given a reward function R(T), the job of the reinforcement learning algorithm is to control\\n\\u200b \\u200b\\nthe helicopter so that it achieves max R(T). However, reinforcement learning algorithms\\n\\u200bT \\u200b\\n\\u200b\\nmake many approximations and may not succeed in achieving this maximization.\\nSuppose you have picked some reward R(.) and have run your learning algorithm. However,\\n\\u200b \\u200b\\nits performance appears far worse than your huma',\n",
       "   1.2958387776508895),\n",
       "  ('ood formal definition of what makes a task easy or\\nhard.16 With the rise of deep learning and multi-layered neural networks, we sometimes say a\\ntask is “easy” if it can be carried out with fewer computation steps (corresponding to a\\nshallow neural network), and “hard” if it requires more computation steps (requiring a\\ndeeper neural network). But these are informal definitions.\\n16Information theory has the concept of “Kolmogorov Complexity”, which says that the complexity of a learned function\\nis the length of the shortest computer program that can produce that function. However, this theoretical concept has found\\nfew practical applications in AI. See also: https://en.wikipedia.org/wiki/Kolmogorov_complexity\\nPage 99 Machine Learning Yearning-Draft Andrew Ng If you are able to take a complex task, and break it down into simpler sub-tasks, then by\\ncoding in the steps of the sub-tasks explicitly, you are giving the algorithm prior knowledge\\nthat can help it learn a task more efficiently.\\nS',\n",
       "   1.2874170938445437),\n",
       "  ('e Learning Yearning-Draft Andrew Ng', 1.2866951126055985),\n",
       "  ('igned audio features. Although they provide a reasonable\\nsummary of the audio input, they also simplify the input signal by throwing some\\ninformation away.\\n• Phonemes are an invention of linguists. They are an imperfect representation of speech\\nsounds. To the extent that phonemes are a poor approximation of reality, forcing an\\nalgorithm to use a phoneme representation will limit the speech system’s performance.\\nThese hand-engineered components limit the potential performance of the speech system.\\nHowever, allowing hand-engineered components also has some advantages:\\n• The MFCC features are robust to some properties of speech that do not affect the content,\\nsuch as speaker pitch. Thus, they help simplify the problem for the learning algorithm.\\n• To the extent that phonemes are a reasonable representation of speech, they can also help\\nthe learning algorithm understand basic sound components and therefore improve its\\nperformance.\\nHaving more hand-engineered components generally allows a s',\n",
       "   1.2790164051379904)]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_augmented_qa_pipeline.run_pipeline(\"How to clean up mislabeled dev and test set examples?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity #1:\n",
    "Enhance your RAG application in some way!\n",
    "- Allow it to work with PDF files\n",
    "- Implement a new distance metric\n",
    "- Add metadata support to the vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visibility Tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install -qU wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "wandb_key = getpass.getpass(\"Weights and Biases API Key: \")\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msyedwaseemjan\u001b[0m (\u001b[33mtest11111111111\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41a9cb3fd8e493096dd100af29efbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011119961677791758, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/waseem/code/AIM3/aim_exercises/exercise_3/wandb/run-20240613_185134-euxtl6ao</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/test11111111111/Visibility%20Example%20-%20AIE3/runs/euxtl6ao' target=\"_blank\">likely-galaxy-5</a></strong> to <a href='https://wandb.ai/test11111111111/Visibility%20Example%20-%20AIE3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/test11111111111/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/test11111111111/Visibility%20Example%20-%20AIE3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/test11111111111/Visibility%20Example%20-%20AIE3/runs/euxtl6ao' target=\"_blank\">https://wandb.ai/test11111111111/Visibility%20Example%20-%20AIE3/runs/euxtl6ao</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/test11111111111/Visibility%20Example%20-%20AIE3/runs/euxtl6ao?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x12cc7a150>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Visibility Example - AIE3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from wandb.sdk.data_types.trace_tree import Trace\n",
    "\n",
    "class RetrievalAugmentedGenerationPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase, wandb_project = None) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.wandb_project = wandb_project\n",
    "\n",
    "    def run_pipeline(self, user_query: str) -> str:\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
    "\n",
    "        context_prompt = \"\"\n",
    "        for context in context_list:\n",
    "            context_prompt += context[0] + \"\\n\"\n",
    "\n",
    "        formatted_system_prompt = rag_prompt.create_message()\n",
    "\n",
    "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
    "\n",
    "\n",
    "        start_time = datetime.datetime.now().timestamp() * 1000\n",
    "\n",
    "        try:\n",
    "            openai_response = self.llm.run([formatted_system_prompt, formatted_user_prompt], text_only=False)\n",
    "            end_time = datetime.datetime.now().timestamp() * 1000\n",
    "            status = \"success\"\n",
    "            status_message = (None, )\n",
    "            response_text = openai_response.choices[0].message.content\n",
    "            token_usage = dict(openai_response.usage)\n",
    "            model = openai_response.model\n",
    "\n",
    "        except Exception as e:\n",
    "            end_time = datetime.datetime.now().timestamp() * 1000\n",
    "            status = \"error\"\n",
    "            status_message = str(e)\n",
    "            response_text = \"\"\n",
    "            token_usage = {}\n",
    "            model = \"\"\n",
    "\n",
    "        if self.wandb_project:\n",
    "            root_span = Trace(\n",
    "                name=\"root_span\",\n",
    "                kind=\"llm\",\n",
    "                status_code=status,\n",
    "                status_message=status_message,\n",
    "                start_time_ms=start_time,\n",
    "                end_time_ms=end_time,\n",
    "                metadata={\n",
    "                    \"token_usage\" : token_usage,\n",
    "                    \"model_name\" : model\n",
    "                },\n",
    "                inputs= {\"system_prompt\" : formatted_system_prompt, \"user_prompt\" : formatted_user_prompt},\n",
    "                outputs= {\"response\" : response_text}\n",
    "            )\n",
    "\n",
    "            root_span.log(name=\"openai_trace\")\n",
    "        \n",
    "        return {\"response\" : response_text, \"context\" : context_list} if response_text else \"We ran into an error. Please try again later. Full Error Message: \" + status_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_augmented_qa_pipeline = RetrievalAugmentedGenerationPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai,\n",
    "    wandb_project=\"LLM Visibility Example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"I don't know.\",\n",
       " 'context': [('ls.\\nNow suppose your training set has 100 examples. Perhaps even a few examples are\\nmislabeled, or ambiguous—some images are very blurry, so even humans cannot tell if there\\nis a cat. Perhaps the learning algorithm can still “memorize” most or all of the training set,\\nbut it is now harder to obtain 100% accuracy. By increasing the training set from 2 to 100\\nexamples, you will find that the training set accuracy will drop slightly.\\nFinally, suppose your training set has 10,000 examples. In this case, it becomes even harder\\nfor the algorithm to perfectly fit all 10,000 examples, especially if some are ambiguous or\\nmislabeled. Thus, your learning algorithm will do even worse on this training set.\\nLet’s add a plot of training error to our earlier figures:\\nYou can see that the blue “training error” curve increases with the size of the training set.\\nFurthermore, your algorithm usually does better on the training set than on the dev set; thus\\nthe red dev error curve usually lies strictly abov',\n",
       "   1.4758253481449088),\n",
       "  ('table for your problem: This technique can affect both bias and variance.\\nPage 54 Machine Learning Yearning-Draft Andrew Ng Learning curves\\nPage 55 Machine Learning Yearning-Draft Andrew Ng 28 Diagnosing bias and variance: Learning\\ncurves\\nWe’ve seen some ways to estimate how much error can be attributed to avoidable bias vs.\\nvariance. We did so by estimating the optimal error rate and computing the algorithm’s\\ntraining set and dev set errors. Let’s discuss a technique that is even more informative:\\nplotting a learning curve.\\nA learning curve plots your dev set error against the number of training examples. To plot it,\\nyou would run your algorithm using different training set sizes. For example, if you have\\n1,000 examples, you might train separate copies of the algorithm on 100, 200, 300, …, 1000\\nexamples. Then you could plot how dev set error varies with the training set size. Here is an\\nexample:\\nAs the training set size increases, the dev set error should decrease.\\nWe will often have ',\n",
       "   1.4622921304467726),\n",
       "  ('  Machine Learning Yearning is a\\ndeeplearning.ai project.\\n© 2018 Andrew Ng. All Rights Reserved.\\nPage 2 Machine Learning Yearning-Draft Andrew Ng Table of Contents\\n1 Why Machine Learning Strategy\\n2 How to use this book to help your team\\n3 Prerequisites and Notation\\n4 Scale drives machine learning progress\\n5 Your development and test sets\\n6 Your dev and test sets should come from the same distribution\\n7 How large do the dev/test sets need to be?\\n8 Establish a single-number evaluation metric for your team to optimize\\n9 Optimizing and satisficing metrics\\n10 Having a dev set and metric speeds up iterations\\n11 When to change dev/test sets and metrics\\n12 Takeaways: Setting up development and test sets\\n13 Build your first system quickly, then iterate\\n14 Error analysis: Look at dev set examples to evaluate ideas\\n15 Evaluating multiple ideas in parallel during error analysis\\n16 Cleaning up mislabeled dev and test set examples\\n17 If you have a large dev set, split it into two subsets, only one o',\n",
       "   1.4609258304576411),\n",
       "  ('o reduce both bias and\\nvariance in your algorithm.\\nPage 62 Machine Learning Yearning-Draft Andrew Ng 32 Plotting learning curves\\nSuppose you have a very small training set of 100 examples. You train your algorithm using a\\nrandomly chosen subset of 10 examples, then 20 examples, then 30, up to 100, increasing\\nthe number of examples by intervals of ten. You then use these 10 data points to plot your\\nlearning curve. You might find that the curve looks slightly noisy (meaning that the values\\nare higher/lower than expected) at the smaller training set sizes.\\nWhen training on just 10 randomly chosen examples, you might be unlucky and have a\\nparticularly “bad” training set, such as one with many ambiguous/mislabeled examples. Or,\\nyou might get lucky and get a particularly “good” training set. Having a small training set\\nmeans that the dev and training errors may randomly fluctuate.\\nIf your machine learning application is heavily skewed toward one class (such as a cat\\nclassification task where',\n",
       "   1.4593942172201195)]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_augmented_qa_pipeline.run_pipeline(\"Who is Batman?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"I don't know.\",\n",
       " 'context': [('ata that came from the same distribution as the dev/test set (mobile images).\\nPage 76 Machine Learning Yearning-Draft Andrew Ng 40 Generalizing from the training set to the\\ndev set\\nSuppose you are applying ML in a setting where the training and the dev/test distributions\\nare different. Say, the training set contains Internet images + Mobile images, and the\\ndev/test sets contain only Mobile images. However, the algorithm is not working well: It has\\na much higher dev/test set error than you would like. Here are some possibilities of what\\nmight be wrong:\\n1. It does not do well on the training set. This is the problem of high (avoidable) bias on the\\ntraining set distribution.\\n2. It does well on the training set, but does not generalize well to previously unseen data\\ndrawn from the same distribution as the training set. This is high variance.\\n\\u200b\\n3. It generalizes well to new data drawn from the same distribution as the training set, but\\nnot to data drawn from the dev/test set distribution. W',\n",
       "   1.3873736813157342),\n",
       "  ('k, as well as other factors that we just don’t\\nunderstand yet.) This makes the academic study of training and testing on different distributions\\ndifficult to carry out in a systematic way.\\nPage 71 Machine Learning Yearning-Draft Andrew Ng But in the era of big data, we now have access to huge training sets, such as cat internet\\nimages. Even if the training set comes from a different distribution than the dev/test set, we\\nstill want to use it for learning since it can provide a lot of information.\\nFor the cat detector example, instead of putting all 10,000 user-uploaded images into the\\ndev/test sets, we might instead put 5,000 into the dev/test sets. We can put the remaining\\n5,000 user-uploaded examples into the training set. This way, your training set of 205,000\\nexamples contains some data that comes from your dev/test distribution along with the\\n200,000 internet images. We will discuss in a later chapter why this method is helpful.\\nLet’s consider a second example. Suppose you are bui',\n",
       "   1.3844006289241524),\n",
       "  ('v set\\n11\\nand test set all come from the same distribution. In the early days of machine learning, data\\nwas scarce. We usually only had one dataset drawn from some probability distribution. So\\nwe would randomly split that data into train/dev/test sets, and the assumption that all the\\ndata was coming from the same source was usually satisfied.\\n11 There is some academic research on training and testing on different distributions. Examples\\ninclude “domain adaptation,” “transfer learning” and “multitask learning.” But there is still a huge\\ngap between theory and practice. If you train on dataset A and test on some very different type of data\\nB, luck could have a huge effect on how well your algorithm performs. (Here, “luck” includes the\\nresearcher’s hand-designed features for the particular task, as well as other factors that we just don’t\\nunderstand yet.) This makes the academic study of training and testing on different distributions\\ndifficult to carry out in a systematic way.\\nPage 71 Mac',\n",
       "   1.3833723084582246),\n",
       "  ('echanical Turk) to\\nobtain even larger datasets. It is thus relatively easy to obtain training data to build a car\\ndetector and a pedestrian detector.\\nIn contrast, consider a pure end-to-end approach:\\nTo train this system, we would need a large dataset of (Image, Steering Direction) pairs. It is\\nvery time-consuming and expensive to have people drive cars around and record their\\nsteering direction to collect such data. You need a fleet of specially-instrumented cars, and a\\nhuge amount of driving to cover a wide range of possible scenarios. This makes an\\nend-to-end system difficult to train. It is much easier to obtain a large dataset of labeled car\\nor pedestrian images.\\nMore generally, if there is a lot of data available for training “intermediate modules” of a\\npipeline (such as a car detector or a pedestrian detector), then you might consider using a\\nPage 97 Machine Learning Yearning-Draft Andrew Ng pipeline with multiple stages. This structure could be superior because you could use al',\n",
       "   1.381758103507781)]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_augmented_qa_pipeline.run_pipeline(\"What are some tips for being an effective CEO?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #5:\n",
    "What is the model_name from the WandB root_span trace?\n",
    "gpt-3.5-turbo-0125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation Using GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"clarity\" : \"2\", \"faithfulness\" : \"1\", \"correctness\" : \"0\"}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are some tips for making an effective model?\"\n",
    "\n",
    "response = retrieval_augmented_qa_pipeline.run_pipeline(query)\n",
    "\n",
    "print(response[\"response\"])\n",
    "\n",
    "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
    "\n",
    "You should be hyper-critical.\n",
    "\n",
    "Provide scores (out of 10) for the following attributes:\n",
    "\n",
    "1. Clarity - how clear is the response\n",
    "2. Faithfulness - how related to the original query is the response and the provided context\n",
    "3. Correctness - was the response correct?\n",
    "\n",
    "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
    "\n",
    "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
    "\n",
    "evaluation_template = \"\"\"Query: {input}\n",
    "Context: {context}\n",
    "Response: {response}\"\"\"\n",
    "\n",
    "try:\n",
    "    chat_openai = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
    "except:\n",
    "    chat_openai = ChatOpenAI()\n",
    "\n",
    "evaluator_system_prompt = SystemRolePrompt(evaluator_system_template)\n",
    "evaluation_prompt = UserRolePrompt(evaluation_template)\n",
    "\n",
    "messages = [\n",
    "    evaluator_system_prompt.create_message(format=False),\n",
    "    evaluation_prompt.create_message(\n",
    "        input=query,\n",
    "        context=\"\\n\".join([context[0] for context in response[\"context\"]]),\n",
    "        response=response[\"response\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_openai.run(messages, response_format={\"type\" : \"json_object\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
